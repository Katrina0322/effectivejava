{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "from urlparse import urljoin\n",
    "import sqlite3\n",
    "import re\n",
    "ignore_words = set(['the', 'to', 'of', 'and', 'a', 'in', 'is', 'it'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /usr/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html5lib\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "page = 'https://baike.baidu.com/item/%E5%A3%B0%E4%B9%90'\n",
    "c= urllib2.urlopen(page)\n",
    "content = c.read()\n",
    "soup = BeautifulSoup(content)\n",
    "links = soup('a')\n",
    "for link in links:\n",
    "    if 'href' in dict(link.attrs):\n",
    "        url = urljoin(page, link['href'])\n",
    "        if url.find(\";\") != -1:\n",
    "            continue\n",
    "        url = url.split('#')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crawler:\n",
    "    def __init__(self, dbname):\n",
    "        self.con = sqlite3.connect(dbname)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.con.close()\n",
    "\n",
    "    def db_commit(self):\n",
    "        self.con.commit()\n",
    "\n",
    "    # 获取id ,如果不存在，加入数据库\n",
    "    def get_entryid(self, table, field, value, createnew=True):\n",
    "        cur = self.con.execute(\"select rowid from %s where %s='%s'\" % (table, field, value))\n",
    "        res = cur.fetchone()\n",
    "        if res==None:\n",
    "            cur = self.con.execute(\"insert into %s (%s) values ('%s')\" % (table, field, value))\n",
    "            return cur.lastrowid\n",
    "        else:\n",
    "            return res[0]\n",
    "\n",
    "    # 建索引\n",
    "    def add_to_index(self, url, soup):\n",
    "        if self.is_indexed(url):\n",
    "            return\n",
    "        print 'Indexing ' + url\n",
    "        text = self.get_text_only(soup)\n",
    "        words = self.get_saperate_words(text)\n",
    "        urlid = self.get_entryid('urllist', 'url', url)\n",
    "        for i in range(len(words)):\n",
    "            word = words[i]\n",
    "            if word in ignore_words:continue\n",
    "            wordid = self.get_entryid('wordlist', 'word', word)\n",
    "            self.con.execute(\"insert into wordlocation(urlid,wordid,location) values (%d,%d,%d)\" % (urlid,wordid,i))\n",
    "\n",
    "    # 从html网页中获取文字\n",
    "    def get_text_only(self, soup):\n",
    "        v = soup.string\n",
    "        if v == None:\n",
    "            c = soup.contents\n",
    "            result_text = ''\n",
    "            for t in c:\n",
    "                subtext = self.get_text_only(t)\n",
    "                result_text += subtext + '\\n'\n",
    "            return result_text\n",
    "        else:\n",
    "            return v.strip()\n",
    "\n",
    "    # 分词处理\n",
    "    def get_saperate_words(self, text):\n",
    "        splitter = re.compile('\\\\W*')\n",
    "        return [s.lower() for s in splitter.split(text) if s != '']\n",
    "\n",
    "    def is_indexed(self, url):\n",
    "        u = self.con.execute(\"select rowid from urllist where url = '%s'\" % url).fetchone()\n",
    "        if u != None:\n",
    "            v = self.con.execute(\"select * from wordlocation where urlid = '%s'\" % url)\n",
    "            if v!= None:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def add_link_ref(self, urlFrom, urlTo, linkText):\n",
    "        pass\n",
    "\n",
    "    def crawl(self, pages, depth=2):\n",
    "        for i in range(depth):\n",
    "            new_pages = set()\n",
    "            for page in pages:\n",
    "                try:\n",
    "                    c = urllib2.urlopen(page)\n",
    "                except:\n",
    "                    print \"Could not open %s \" % page\n",
    "                    continue\n",
    "                soup = BeautifulSoup(c.read())\n",
    "                self.add_to_index(page, soup)\n",
    "                links = soup('a')\n",
    "                for link in links:\n",
    "                    if 'href' in dict(link.attrs):\n",
    "                        url = urljoin(page, link['href'])\n",
    "                        if url.find(\";\") != -1:\n",
    "                            continue\n",
    "                        url = url.split(\"#\")[0]\n",
    "                        if url[0:4]=='http' and not self.is_indexed(url):\n",
    "                            new_pages.add(url)\n",
    "                        link_text = self.get_text_only(link)\n",
    "                        self.add_link_ref(page, url, link_text)\n",
    "                self.db_commit()\n",
    "            pages = new_pages\n",
    "\n",
    "    def create_index_tables(self):\n",
    "        self.con.execute('create table if not exists urllist(url) ')\n",
    "        self.con.execute('create table if not exists wordlist(word)')\n",
    "        self.con.execute('create table if not exists wordlocation(urlid,wordid,location)')\n",
    "        self.con.execute('create table if not exists link(fromid integer,toid integer)')\n",
    "        self.con.execute('create table if not exists linkwords(wordid,linkid)')\n",
    "        self.con.execute('create index if not exists wordidx on wordlist(word)')\n",
    "        self.con.execute('create index if not exists urlidx on urllist(url)')\n",
    "        self.con.execute('create index if not exists wordurlidx on wordlocation(wordid)')\n",
    "        self.con.execute('create index if not exists urltoidx on link(toid)')\n",
    "        self.con.execute('create index if not exists urlfromidx on link(fromid)')\n",
    "        self.db_commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = ['https://baike.baidu.com/item/%E5%A3%B0%E4%B9%90','https://www.zhihu.com/topic/19550228/hot']\n",
    "crawler = Crawler('searchindex.db')\n",
    "# crawler.create_index_tables()\n",
    "# crawler.crawl(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Searcher:\n",
    "    def __init__(self, dbname):\n",
    "        self.con = sqlite3.connect(dbname)\n",
    "        \n",
    "    def __del__(self):\n",
    "        self.con.close()\n",
    "      \n",
    "    def get_match_rows(self, q):\n",
    "        field_list = 'w0.urlid'\n",
    "        table_list = ''\n",
    "        clause_list = ''\n",
    "        word_ids = []\n",
    "        \n",
    "        words = q.split(' ')\n",
    "        table_number = 0\n",
    "        for word in words:\n",
    "            word_row = self.con.execute(\"select rowid from wordlist where word='%s'\" % word).fetchone()\n",
    "            if word_row is not None:\n",
    "                word_id = word_row[0]\n",
    "                word_ids.append(word_id)\n",
    "                if table_number > 0:\n",
    "                    table_list += ','\n",
    "                    clause_list += ' and '\n",
    "                    clause_list += 'w%d.urlid=w%d.urlid and ' % (table_number -1, table_number)\n",
    "                field_list += ',w%d.location' % table_number\n",
    "                table_list += 'wordlocation w%d' % table_number\n",
    "                clause_list += 'w%d.wordid = %d' % (table_number, word_id)\n",
    "        full_query = 'select %s from %s where %s' % (field_list, table_list, clause_list)\n",
    "        cur = self.con.execute(full_query)\n",
    "        rows = [row for wor in cur]\n",
    "        return rows, word_ids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
