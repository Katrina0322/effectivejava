{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "from urlparse import urljoin\n",
    "import sqlite3\n",
    "ignore_words = set(['the', 'to', 'of', 'and', 'a', 'in', 'is', 'it'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"/contact/apply;JSESSIONID=a3d71372-289a-40c1-905d-813d4b91bc54\">\\u8bd5\\u7528\\u7533\\u8bf7</a>, <a href=\"javascript:void(0)\"><label><input class=\"rememberpwd\" type=\"checkbox\"/> \\u8bb0\\u4f4f\\u5bc6\\u7801</label></a>, <a class=\"btn\">\\u786e\\u5b9a</a>]\n",
      "http://inewsengine.com/contact/apply;JSESSIONID=a3d71372-289a-40c1-905d-813d4b91bc54\n",
      "javascript:void(0)\n"
     ]
    }
   ],
   "source": [
    "page = 'http://inewsengine.com/uec/latest/front/news/list'\n",
    "c= urllib2.urlopen(page)\n",
    "content = c.read()\n",
    "soup = BeautifulSoup(content)\n",
    "links = soup('a')\n",
    "print links\n",
    "for link in links:\n",
    "    if 'href' in dict(link.attrs):\n",
    "        url = urljoin(page, link['href'])\n",
    "        if url.find(\"'\") != -1:\n",
    "            continue\n",
    "        url = url.split('#')[0]\n",
    "        print url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crawler:\n",
    "    def __init__(self, dbname):\n",
    "        self.con = sqlite3.connect(dbname)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.con.close()\n",
    "\n",
    "    def db_commit(self):\n",
    "        self.con.commit()\n",
    "\n",
    "    # 获取id ,如果不存在，加入数据库\n",
    "    def get_entryid(self, table, field, value, createnew=True):\n",
    "        cur = self.con.execute(\"select rowid from %s where %s='%s'\" % (table, field, value))\n",
    "        res = cur.fetchone()\n",
    "        if res==None:\n",
    "            cur = self.con.execute(\"insert into %s (%s) values ('%s')\" % (table, field, value))\n",
    "            return cur.lastrowid\n",
    "        else:\n",
    "            return res[0]\n",
    "\n",
    "    # 建索引\n",
    "    def add_to_index(self, url, soup):\n",
    "        if self.is_indexed(url):\n",
    "            return\n",
    "        print 'Indexing ' + url\n",
    "        text = self.get_text_only(soup)\n",
    "        words = self.get_saperate_words(text)\n",
    "        urlid = self.get_entryid('urllist', 'url', url)\n",
    "        for i in range(len(words)):\n",
    "            word = words[i]\n",
    "            if word in ignore_words:continue\n",
    "            wordid = self.get_entryid('wordlist', 'word', word)\n",
    "            self.con.execute(\"insert into wordlocation(urlid,wordid,location) values (%d,%d,%d)\" % (urlid,wordid,i))\n",
    "\n",
    "    # 从html网页中获取文字\n",
    "    def get_text_only(self, soup):\n",
    "        v = soup.string\n",
    "        if v == None:\n",
    "            c = soup.contents\n",
    "            result_text = ''\n",
    "            for t in c:\n",
    "                subtext = self.get_text_only(t)\n",
    "                result_text += subtext + '\\n'\n",
    "            return result_text\n",
    "        else:\n",
    "            return v.strip()\n",
    "\n",
    "    # 分词处理\n",
    "    def get_saperate_words(self, text):\n",
    "        splitter = re.compile('\\\\W*')\n",
    "        return [s.lower() for s in splitter.split(text) if s != '']\n",
    "\n",
    "    def is_indexed(self, url):\n",
    "        u = self.con.execute(\"select rowid from urllist where url = '%s'\" % url).fetchone()\n",
    "        if u != None:\n",
    "            v = self.con.execute(\"select * from wordlocation where urlid = '%s'\" % url)\n",
    "            if v!= None:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def add_link_ref(self, urlFrom, urlTo, linkText):\n",
    "        pass\n",
    "\n",
    "    def crawl(self, pages, depth=2):\n",
    "        for i in range(depth):\n",
    "            new_pages = set()\n",
    "            for page in pages:\n",
    "                try:\n",
    "                    c = urllib2.urlopen(page)\n",
    "                except:\n",
    "                    print \"Could not open %s \" % page\n",
    "                    continue\n",
    "                soup = BeautifulSoup(c.read())\n",
    "                self.add_to_index(page, soup)\n",
    "                links = soup('a')\n",
    "                for link in links:\n",
    "                    if 'href' in dict(link.attrs):\n",
    "                        url = urljoin(page, link['href'])\n",
    "                        if url.find(\"'\") != -1:\n",
    "                            continue\n",
    "                        url = url.split(\"#\")[0]\n",
    "                        if url[0:4]=='http' and not self.is_indexed(url):\n",
    "                            new_pages.add(url)\n",
    "                        link_text = self.get_text_only(link)\n",
    "                        self.add_link_ref(page, url, link_text)\n",
    "                self.db_commit()\n",
    "            pages = new_pages\n",
    "\n",
    "    def create_index_tables(self):\n",
    "        self.con.execute('create table urllist(url)')\n",
    "        self.con.execute('create table wordlist(word)')\n",
    "        self.con.execute('create table wordlocation(urlid,wordid,location)')\n",
    "        self.con.execute('create table link(fromid integer,toid integer)')\n",
    "        self.con.execute('create table linkwords(wordid,linkid)')\n",
    "        self.con.execute('create index wordidx on wordlist(word)')\n",
    "        self.con.execute('create index urlidx on urllist(url)')\n",
    "        self.con.execute('create index wordurlidx on wordlocation(wordid)')\n",
    "        self.con.execute('create index urltoidx on link(toid)')\n",
    "        self.con.execute('create index urlfromidx on link(fromid)')\n",
    "        self.dbcommit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
